# Oral Session 1: Speech Technology

## Fatigue and sleepiness detection based on speech analysis
### Carla Vasconcelos, Maurílio Nunes Vieira, Hani Camille Yehia

Background:  Mental   fatigue   and   sleepiness   are   well   recognized   determinants   of  human-error related accidents and incidents in aviation. In Brazil, according to CENIPA    (Center for   Investigation   and   Prevention   of   Aeronautical   Accidents),   the   rate   of    accidents in the aerial modal is of 1 per 2 days. Human factors are present in 90% of    these   accidents.  Case   Report:  This   paper   describes   a   retrospective   study   of   the communication   between a pilot   and   an   air  traffic   control   tower   just  before   a   fatal    accident. The  objective  was the  detection of   fatigue and  sleepiness  of  a pilot,  who    complained   of  these signs  and   symptoms  before   the  flight,  by  means  of voice  and    speech analysis. The in-depth accident analysis performed by CENIPA indicated that    sleepiness and fatigue most likely contributed to the accident. Speech samples were analyzed for two conditions: (i) non-sleepy data recorded thirty-five hours before the air crash (control condition), which were compared with (ii) data from samples collected    about  one   hour  before   the  accident  and  also during  the   disaster  (sleepy  condition). Audio recordings were analyzed for the extraction of objective measures of the temporal organization of speech, such as hesitations, silent pauses, prolongation of final syllables, and syllable articulation rate. Discussion: The results showed that speech during the day  of the accident had a ignificantly low elocution and articulation rates compared to the    preceding day, indicating also that the methodology adopted in this study is feasible for detection of fatigue and sleepiness through speech analysis. This seems to be the first such application in aeronautical accidents.


## Articulatory Measures of Speech Production using NDI Wave Articulograph
### Gabriela Chaltein, Melchior Augusto Syrio de Melo, Hani Camille Yehia

This project aims to elaborate and implement a methodology based on modern measurement techniques to analyze existing relations between processes of production and perception of human speech. Instead of investigating the process of perception through operations done directly over the speech signal, such as filtering and adding noise, this project focus on altering the articulatory parameters of speech production, in order to analyze the perception process in a `layer` above the acoustic signal. The methodology starts with articulatory measurements carried out with an NDI Wave articulograph. These measurements are the base to define articulatory gestures effectively used by speakers and their relation with the speech sounds produced.
The results obtained from articulatory measurements will be then combined with human speech perception measurements based on responses to complex sounds evoked by the auditory brain stem. This is done by using the articulatory parameters that have been measured to synthesize the complex sounds used as stimuli to obtain evoked potentials. Besides that, categorizing tests will be used to associate evoked potential measurements and speech production configurations with the conscious perception capacity of variations in the signals used as stimuli. This presentation has a focus on analyzing articulatory measurements using the NDI Wave articulograph.

## Auditory perceived distance model for the ITU loudness algorithm
### Leandro da Silva Pires, Hani Camille Yehia, Maurílio Nunes Vieira

The International Telecommunication Union, Radiocommunication Sector (ITU-R) Recommendation BS.1770 for loudness measurement in mutichannel audio is established as a de facto standard for audio companies and de jure for digital broadcasters. Although its frequency weighting accounts for acoustic effects of the head, the model is insensitive to source distance. Listening tests were undertook to investigate the effect of auditory distance perception on loudness of noise, speech, music and environmental sounds. Based on the variations found, an adaptation of the ITU-R algorithm is proposed and evaluated against subject responses. Resulting differences in loudness levels were within the confidence intervals of the level differences indicated by the subjects in source-reciever distances commonly found in living rooms.

## Corpus CEFALA 1: Audiovisual database of speakers for biometric, phonetic and phonology studies
### Arlindo Follador Neto, Adelino Pinheiro Silva, Hani Camille Yehia

Human speech has been studied in different areas of knowledge, which range from biometry to phonetics and phonology. In the research conducted in such areas, speech samples are necessary resources for obtaining results and validating hypotheses. For this, samples of different speakers and contents are stored in audio files and organized into databases. Such databases allow the continuity, practicality and reliability of studies, eliminating the difficult and time consuming step of data collection. Moreover, they allow consistent comparisons between different studies. However, free access databases in the Portuguese language or recorded in controlled environments are rarely found. The objective of this research is to construct a free and public database of Brazilian Portuguese, named Corpus CEFALA-1. The database comprising 104 speakers guided by a specific protocol for the collection of audiovisual speech samples recorded in a studio. The study presents the methodologies of processing, segmentation and organization of speech samples, statistical analysis, application to biometric verification and preliminary phonetic-phonological analyses.


